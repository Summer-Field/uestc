# 数据分析算法 --  数据关系

## 概述

数据分析是从海量数据中提取信息的过程，以机器学习算法为基础，通过模拟人类的学习行为，获取新的知识和既能，不断改善分析的过程

机器学习从很多学科中吸收了重要的成果

## 5.1 TF-IDF算法

- NLP中最经典的应用是在**一堆文档中选择属于每个文本最具有代表性的词汇**
  - 关键字、摘要
- TF-IDF(Term Frequence-Inverse Document Frequency)



- 什么是**关键字**？
  - 对文档的高度**概括**
  - **区分**不同的文档

### 5.1.1 词袋模型

- 文本进入计算的第一步：将**文本转换到空间**（可以计算的数量）
- 向量空间模型  --  将文本映射到向量中表示空间
  - 词袋模型：广泛用于**NLP处理和信息检索**的词语模型
  - 将若干词语放到一个袋子中，不考虑词语之间的语法和相互顺序

| **文本** | **语句**                               |
| -------- | -------------------------------------- |
| **A**    | 小张 喜欢   打 篮球 和   打 羽毛球     |
| **B**    | 小李 喜欢   打 羽毛球                  |
| **词袋** | 小张、喜欢、打、篮球、和、羽毛球、小李 |

---

| **文本**  | **语句**                                                    |
| --------- | ----------------------------------------------------------- |
| **A**     | 小张(1)、喜欢(1)、打(2)、篮球(1)、和(1)、羽毛球(1)、小李(0) |
| **A向量** | 1  1 2 1 1 1 0                                              |
| **B**     | 小张(0)、喜欢(1)、打(1)、篮球(0)、和(0)、羽毛球(1)、小李(1) |
| **B向量** | 0  1 1 0 0 1 1                                              |

词袋模型简单、易于理解，但是其假设句子和**语法与词序**无关，不符合自然语言的**实际分布规则和含义**，因此不能进行更深层次的语义处理。

### 5.1.2 TF-IDF算法

#### 基本思想

文档中每个词的重要性和它在当前文档中出现的次数成正比，但是和它在其他文件中出现的次数成反比

- 推论

如果一个词语在某个文档中出现的频率很高，并且在其他文档集合中出现的频率很低，那么则认为该词语对于文件A有一定的代表性，能够通过该词语与其他的文档形成较好的内容区分能力

#### 适用场景

**擅长的是与词频相关、忽略词序和语法的文本信息处理**

#### 算法

$$
TF−IDF_(𝑖,𝑗)=TF_(𝑖,𝑗)×𝐼𝐷𝐹_𝑖=𝑛_(𝑖,𝑗)/(∑_𝑘𝑛_(𝑘,𝑗))×log⁡_|𝐷|/({𝑗:𝑡_𝑖∈𝑑_𝑗})
$$

| **文档** | **文档中对应词语集合**                   |
| -------- | ---------------------------------------- |
| **A**    | 努力 向前 **奋斗 奋斗** 使得 未来   更好 |
| **B**    | 创新 创新   万众 智慧                    |
| **C**    | **奋斗** 是   人生 的 一部分             |

$$
𝑡_𝑖="奋斗", 𝑛_(𝑖,1)=2, 𝑛_(𝑖,2)=0, 𝑛_(𝑖,3)=1, ∑_𝑘𝑛_(𝑘,1) =7,∑_𝑘𝑛_(𝑘,2) =4,∑_𝑘𝑛_(𝑘,3) =5

$$

$$
文档数|𝐷|=3，包含“奋斗”的文档{𝑗:𝑡_𝑖∈𝑑_𝑗}=2
$$



- ti  --  具体某一个单词
- n(i,1)：ti在文档1中出现的次数
- tj  --  哪一个文档
- (i，j):第j个文档中的第i个term
- j:ti∈dj：包含ti的文本个数

>  **遍历每个词语，值最大的适宜做关键词或区分词**

#### 缺点

- 对短文本的处理和过长文本的处理不是很好
- 忽视了文档中语义和语法的表达
- 词语之间必须完全匹配，对**相似词语**或者**词语的子词语**不能进行有效的匹配

## 5.2 余弦相似性

- 问题：如何判断两个数据（文本）的相似性？
- 例如：
  - 数据价值是一种数据艺术
  - 算法价值是一种算法艺术

### Jaccard系数

用于个体的特征属性通过**符号度量**或者**布尔标识符**，适合**集合的计算**

对于文本，最直观的想法是：两篇文章足够相似，那么他们赐予的交集越多
$$
𝐽(𝐴,𝐵)=(|𝐴∩𝐵|)/(|𝐴∪𝐵|)=(|𝐴∩𝐵|)/(|𝐴|+|𝐵|−|𝐴∩𝐵|)
$$

$$
𝐸𝐽(𝐴,𝐵)=(𝐴×𝐵)/(‖𝐴‖^2+‖𝐵‖^2−𝐴×𝐵)
$$

- Jaccard的理论基础支持不够，因为仅依靠是否出现去判断两者相似度不够精确
- 因此引入了**余弦相似性**，通过预先的方式计算相似度，将词语是否出现变更为词语在文本中的权重

$$
cos⁡𝜃=(𝑎×𝑏)/(‖𝑎‖×‖𝑏‖ )=(∑_𝑖^𝑛(𝑥_𝑖×𝑦_𝑖))/(√∑_𝑖^𝑛𝑥_𝑖^2 * √∑_𝑖^𝑛𝑦_𝑖^2 )
$$

#### Jaccard相似性的缺点

- 只关心个体间特征属性是否相同，反映了样本交集和并集之间的差异
- AB文本相似词语，但是意思迥异，余弦相似度更符合实际情况



优点：简单有效

缺点：数据纬度高，计算复杂对越高，不适应当前的大数据

## 5.4 Apriori算法

- 数据的关联规则用于从看似**无关的海量历史数据**中挖掘出可能具有价值信息，在商业活动中，会利用**数据之间的关系**产生比较大的商业价值
- **关联规则**反映的是两个或者多个事物之间相互**依存性和关联性**

- 例子：在超市中，如何解决根据客户的历史信息来优化货物摆放位置
  - 频繁项集算法
  - 广泛用于超市商品关联分析、消费习惯分析
  - 利用频繁项集的先验知识，不断地按照层次进行迭代
  - 利用频繁项集的先验知识，不断地按照层次进行迭代，计算数据集中的所有可能的频繁项集

### 5.4.1 概念

- 项集：项的集合。{牛奶，面包}，牛奶、面包就是项，{牛奶、面包}就2项集

- 关联规则：形如X->Y的蕴含表达式（if...then），其中X，Y是不想相交的项集

- 支持度：项集**X，Y同时发生的概率**称之为关联规则的支持度
  $$
  𝑐(𝑋→𝑌)=(|𝑋∪𝑌|)/(|N|)
  $$
  

- 置信度：项集**X发生的情况下，项集Y发生的概率**，
  $$
  𝑐(𝑋→𝑌)=(|𝑋∪𝑌|)/(|𝑋|)
  $$

- 最小支持度：认为按照实际意义规定的阈值，表示相继在统计意义上的最低重要性

- 最小置信度：认为按照实际意义规定的阈值，表示关联规则的最低可靠性。

> 如果支持度和置信度同时达到最小值尺度和最小置信度，那么此关联规则为强规则

- 频繁项集：支持最小支持度的所有项集

### 5.4.2 Apriori算法核心

- 根据支持度找出频繁项集

频繁可以理解为数据的频率，所筛选出的项集频率不得低于支持度。如果满足最小支持度的频繁项集中包含K个元素，则称作**频繁K项集。**

- 根据置信度产生关联规则

####  Apriori量达定理

- 如果一个集合是频繁项集，那么它的所有子集都是频繁项集合
- 如果一个集合它不是频繁项集合，那么它的所有超集都不是频繁项集

### 5.4.3 算法

1. 扫描历史数据，并对每项数据进行频率次数统计

2. 构建候选项集C1，并计算其支持度

3. 对候选项集的支持度进行筛选，从而形成频繁项集L1

4. 对频繁项集L1进行连接生成候选项集C2

5. 重复上述步骤，最终形成频繁*K*项集或者最大频繁项集

#### 例子

<img src="/Users/bytedance/uestc/big_data/note/pic/Picture6.png" alt="Picture6" style="zoom:50%;" />

<img src="/Users/bytedance/uestc/big_data/note/pic/Picture7.png" alt="Picture6" style="zoom:50%;" />

<img src="/Users/bytedance/uestc/big_data/note/pic/Picture8.png" alt="Picture6" style="zoom:50%;" />

### 5.4.4 算法-输出规则

根据c(X→Y)=|X∪Y|/|X| ，关联规则产生步骤如下：

1、对于每个频繁项集L，产生其所有非空真子集S={s}；

2、对于每个非空真子集s，令Q=L-s表示集合L中去除了s后剩余的集合，如果c(s→Q)大于等于设定的最小置信度，则输出规则s→Q。

### 5.4.5 Apriori算法缺点

- 产生候选项集时产生较多的组合，没有考虑将一些无关的元素排除后再进行组合
- 每次计算项集的过程都会扫描原始数据表，对数据大的系统而言，重复扫描开销大

解决途径

- 压缩数据表使用哈比表快速查找特性对项集统计
- 合理选样
- FP-Growth算法

## 5.5 PageRank算法

- 数据关系还可以应用到搜索引擎和推荐系统
- 思想

看一个人咋么样，看他有什么样的朋友就可以知道了

> 越多被优质网页所指引的页面，他就是优质网页的概率越大

### 两个假设

- 数量假设

一个页面结点接收到的其他页面指向的**入链数量越多**，那么这个页面就越重要

- 质量假设

越是质量高的页面指向页面A，则页面A越重要

### 算法步骤

1. 每个网页初始化的PR值为1/N，N是网页总数

2. 根据投票算法不断迭代
   $$
   𝑃𝑅(𝑢)=∑_𝑣(𝑃𝑅(𝑣))/(𝐿(𝑣)) ---- (𝑣∈𝐵_𝑢)
   $$

   > Bu是所有链接到网页*u* 的网页集合
   >
   > 网页*v* 是Bu中的一个网页
   >
   > L(v)是网页v的出度

|               | PR(A)  | PR(B)  | PR(C)  | PR(D)  |
| ------------- | ------ | ------ | ------ | ------ |
| **初始值**    | 0.25   | 0.25   | 0.25   | 0.25   |
| **第1次迭代** | 0.125  | 0.333  | 0.083  | 0.458  |
| **第2次迭代** | 0.1665 | 0.4997 | 0.0417 | 0.2912 |
| **……**        | ……     | ……     | ……     | ……     |
| **第n次迭代** | 0.1999 | 0.3999 | 0.0666 | 0.3333 |

- 网页没有出度 -- > 排名泄露，所有网页的PR值都趋向于0
  - 强制A对所有网页，包括自己都有出链

<img src="/Users/bytedance/uestc/big_data/note/pic/Picture9.png" alt="Picture6" style="zoom:50%;" />

- 没有入度  -->排名下沉，该网页的PR值趋近于0
- 网页只有对自己有出链，或者几个网页的出链形成封闭—>排名上升，这些网页的PR值只增不减



<img src="/Users/bytedance/uestc/big_data/note/pic/pic10.png" alt="Picture6" style="zoom:50%;" />

<img src="/Users/bytedance/uestc/big_data/note/pic/pic11.png" alt="Picture6" style="zoom:50%;" />

### 优缺点

优点

- 与查询无关的静态算法，所有网页的PageRank值通过离线计算获得；有效减少在线查询时的计算量，极大降低了查询响应时间。

缺点

- 过分相信链接关系，而一些权威网页往往是相互不链接的；忽视了主题相关性；旧的页面等级会比新页面高。

